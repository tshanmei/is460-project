{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3573e5e4",
   "metadata": {},
   "source": [
    "# IS460-G2 Machine Learning & Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead57e4",
   "metadata": {},
   "source": [
    "## Project Topic: Classification and Prediction of Pneumonia from Chest X-Ray Images\n",
    "\n",
    "### Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa369946",
   "metadata": {},
   "source": [
    "#### Group 7: Shaun, Shan Mei, Xiao Xiao, Xiao Fang, Matthew, Tun Hao\n",
    " \n",
    " Following the Severe Pneumonia clusters that raised questions on accuracy and precision of pneumonia diagnosis during the COVID pandemic, this project aims to accurately identify and diagnose pneumonia patients through image classification and explore other methods of building machine learning algorithms to solve the problem.\n",
    " \n",
    "(https://www.straitstimes.com/singapore/health/all-pneumonia-patients-in-public-hospitals-here-being-tested-for-coronavirus-moh) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bafdeb",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c542f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xfaan\\Documents\\GitHub\\is460-project\\myenv\\Scripts\\python.exe\n",
      "0.22.0\n",
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "# # Run Once\n",
    "#!pip install opencv-python\n",
    "#!pip install tensorflow\n",
    "#!pip install --upgrade scikit-image\n",
    "#!pip install numpy scikit-learn\n",
    "\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "import skimage\n",
    "print(skimage.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313716d",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e7f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import keras \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884486aa",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"chest_xray\"\n",
    "folders = ['train','test','val']\n",
    "\n",
    "pd.DataFrame(os.listdir(path),columns=['Files'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f733ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Image Count in Folders\n",
    "dict_={'train':[],'test':[],'val':[]}\n",
    "class_names = []\n",
    "for i in tqdm(folders):\n",
    "    path_ = os.path.join(path, i)\n",
    "    \n",
    "    for j in os.listdir(path_):\n",
    "        if i == 'train':  # Only append class names once, assuming 'train' contains all classes\n",
    "            class_names.append(j)\n",
    "        dict_[i].append(len(os.listdir(os.path.join(path_, j))))\n",
    "\n",
    "df = pd.DataFrame(dict_, index=class_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6825348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "df.plot(kind='bar', figsize=(10,6))\n",
    "plt.title('Distribution of Classes in Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Identify Image Sizes\n",
    "def Size(folder):\n",
    "    size=[]\n",
    "    path_=os.path.join(path,folder)\n",
    "    for i in tqdm(os.listdir(path_)):\n",
    "        path2=os.path.join(path_,i)\n",
    "        for j in os.listdir(path2):\n",
    "            img=cv2.imread(os.path.join(path2,j))\n",
    "            size.append(img.shape)\n",
    "    return pd.Series(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d069b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Image Sizes in Each Folder\n",
    "print(\"Size of train: \", Size('train')); print(\"Size of test: \", Size('test')); print(\"Size of val: \",Size('val'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d209c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425f828",
   "metadata": {},
   "source": [
    "In our Image Classification problem, Data Pre-Processing involves:  \n",
    "  \n",
    "- Rescaling Colors in Images \n",
    "- Resizing Images\n",
    "- Normalizing Pixel Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a93229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Process Dataset\n",
    "Size=80\n",
    "images=[]\n",
    "labels=[]\n",
    "\n",
    "def load():\n",
    "    global images\n",
    "    global labels\n",
    "    \n",
    "    for folder in tqdm(folders):\n",
    "        path_=os.path.join(path,folder)\n",
    "        \n",
    "        for files in os.listdir(path_):\n",
    "            path2=os.path.join(path_,files)\n",
    "            \n",
    "            for img in os.listdir(path2):\n",
    "                # Read Images\n",
    "                image=cv2.imread(os.path.join(path2,img))\n",
    "                # Convert BGR to RGB Color Codes\n",
    "                image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "                # Resize Images\n",
    "                image=cv2.resize(image,(Size,Size))\n",
    "                # Normalize Pixel Values (Scaling to [0, 1])\n",
    "                image=image/255.0\n",
    "                images.append(image)\n",
    "                labels.append(files)      \n",
    "load()\n",
    "\n",
    "images=np.array(images)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of Pre-Processed Data\n",
    "print('Images Shape :',images.shape); print('Labels Shape :',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cd641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Pre-Processed Data\n",
    "plt.figure(figsize=(25,25))\n",
    "x = 1\n",
    "for i in np.random.randint(0,len(images),50):\n",
    "    plt.subplot(10, 5, x)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f'Label {labels[i]}', fontsize=20)\n",
    "    plt.axis('off')    \n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4320e",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631912ac",
   "metadata": {},
   "source": [
    "To build a Naive Bayes Model in this problem, we will need to create numerical features based off these images. In this method, we assume grayscale images _(8-bit range)_ because they are widely used for texture analysis and basic feature extraction in image processing and computer vision. \n",
    "\n",
    "Considering that our dataset are X-Ray Images of Lungs, we build a pandas DataFrame with the following features from these images:  \n",
    "- Histogram of Pixel Intensities - Represents distribution of pixel intensities in the image  \n",
    "- Texture Features (Gray-Level Co-Occurence Matrix) - Measures texture patterns in the X-Ray image  \n",
    "- Statistical Features - Compute statistics such as mean, standard deviation, skewness, and kurtosis of pixel intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f198e",
   "metadata": {},
   "source": [
    "#### Histogram of Pixel Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_histograms = []\n",
    "\n",
    "for image in images:\n",
    "    \n",
    "    # Convert the image data type to uint8\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate Histogram of Pixel Intensities\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    hist = hist.flatten()\n",
    "    \n",
    "    # Append intensity histogram features to the list\n",
    "    intensity_histograms.append(hist)\n",
    "    \n",
    "# Create Pandas DataFrame with intensity histogram features\n",
    "nb_histogram = pd.DataFrame(intensity_histograms)\n",
    "\n",
    "print(nb_histogram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for the first few images\n",
    "num_images_to_plot = 5  \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx in range(num_images_to_plot):\n",
    "    plt.subplot(num_images_to_plot, 1, idx + 1)\n",
    "    plt.plot(nb_histogram.iloc[idx])\n",
    "    plt.title(f'Histogram for Image {idx}')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64942c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Averaging histograms over all images to get a general sense\n",
    "avg_histogram = nb_histogram.mean(axis=0)\n",
    "\n",
    "# Plotting the average histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_histogram)\n",
    "plt.title('Average Histogram of Pixel Intensities for Chest X-ray Images')\n",
    "plt.xlabel('Pixel Intensity Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Statistical measures\n",
    "mean_intensity = nb_histogram.mean(axis=1).mean()\n",
    "median_intensity = nb_histogram.mean(axis=1).median()\n",
    "std_intensity = nb_histogram.mean(axis=1).std()\n",
    "\n",
    "print(f\"Average Mean Intensity Across All Images: {mean_intensity:.2f}\")\n",
    "print(f\"Average Median Intensity Across All Images: {median_intensity:.2f}\")\n",
    "print(f\"Standard Deviation of Intensities Across All Images: {std_intensity:.2f}\")\n",
    "\n",
    "# Bonus: Box plot to understand intensity distribution variability across images\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(nb_histogram.mean(axis=1))\n",
    "plt.title('Distribution of Mean Pixel Intensities Across Chest X-ray Images')\n",
    "plt.ylabel('Mean Pixel Intensity')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26641c41",
   "metadata": {},
   "source": [
    "#### Texture Features (Gray-Level Co-Occurence Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "glcm_contrast = []\n",
    "glcm_correlation = []\n",
    "\n",
    "for image in images:\n",
    "    \n",
    "    # Convert the image data type to uint8\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert the color image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate GLCM\n",
    "    glcm = graycomatrix(gray_image, [1], [0], symmetric=True, normed=True)\n",
    "\n",
    "    # Calculate GLCM features (contrast and correlation)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "\n",
    "    # Append GLCM features and label to respective lists\n",
    "    glcm_contrast.append(contrast)\n",
    "    glcm_correlation.append(correlation)\n",
    "\n",
    "# Create a Pandas DataFrame with GLCM features and labels\n",
    "import pandas as pd\n",
    "\n",
    "nb_glcm = pd.DataFrame({'GLCM_Contrast': glcm_contrast, 'GLCM_Correlation': glcm_correlation})\n",
    "\n",
    "print(nb_glcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9aaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(nb_glcm['GLCM_Contrast'], nb_glcm['GLCM_Correlation'], alpha=0.5)\n",
    "plt.title('Scatter plot of GLCM Texture Features')\n",
    "plt.xlabel('GLCM Contrast')\n",
    "plt.ylabel('GLCM Correlation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for GLCM Contrast\n",
    "mean_contrast = nb_glcm['GLCM_Contrast'].mean()\n",
    "median_contrast = nb_glcm['GLCM_Contrast'].median()\n",
    "std_contrast = nb_glcm['GLCM_Contrast'].std()\n",
    "min_contrast = nb_glcm['GLCM_Contrast'].min()\n",
    "max_contrast = nb_glcm['GLCM_Contrast'].max()\n",
    "\n",
    "# Compute summary statistics for GLCM Correlation\n",
    "mean_correlation = nb_glcm['GLCM_Correlation'].mean()\n",
    "median_correlation = nb_glcm['GLCM_Correlation'].median()\n",
    "std_correlation = nb_glcm['GLCM_Correlation'].std()\n",
    "min_correlation = nb_glcm['GLCM_Correlation'].min()\n",
    "max_correlation = nb_glcm['GLCM_Correlation'].max()\n",
    "\n",
    "# Print the summary\n",
    "print(\"GLCM Contrast Summary:\")\n",
    "print(f\"Mean: {mean_contrast:.2f}\")\n",
    "print(f\"Median: {median_contrast:.2f}\")\n",
    "print(f\"Standard Deviation: {std_contrast:.2f}\")\n",
    "print(f\"Min: {min_contrast:.2f}\")\n",
    "print(f\"Max: {max_contrast:.2f}\")\n",
    "print(\"\\nGLCM Correlation Summary:\")\n",
    "print(f\"Mean: {mean_correlation:.2f}\")\n",
    "print(f\"Median: {median_correlation:.2f}\")\n",
    "print(f\"Standard Deviation: {std_correlation:.2f}\")\n",
    "print(f\"Min: {min_correlation:.2f}\")\n",
    "print(f\"Max: {max_correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert your labels into a DataFrame\n",
    "df_labels = pd.DataFrame({'Diagnosis': labels})\n",
    "\n",
    "# Merge the GLCM features dataframe with the diagnosis labels\n",
    "df = pd.concat([nb_glcm, df_labels], axis=1)\n",
    "\n",
    "# Calculate the correlation matrix for GLCM features and labels\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Since 'labels' are categorical, we'll use one hot encoding or dummy encoding to make it numerical before finding correlation\n",
    "encoded_labels = pd.get_dummies(df_labels['Diagnosis'])\n",
    "\n",
    "# Add the GLCM features to this encoded DataFrame\n",
    "df_encoded = pd.concat([nb_glcm, encoded_labels], axis=1)\n",
    "\n",
    "# Calculate correlation with encoded labels\n",
    "correlation_encoded = df_encoded.corr()\n",
    "\n",
    "# Let's say you have two categories: \"NORMAL\" and \"PNEUMONIA\". Extract their correlations\n",
    "correlation_with_normal = correlation_encoded[\"NORMAL\"].sort_values(ascending=False)\n",
    "correlation_with_pneumonia = correlation_encoded[\"PNEUMONIA\"].sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with NORMAL:\")\n",
    "print(correlation_with_normal)\n",
    "\n",
    "print(\"\\nCorrelation with PNEUMONIA:\")\n",
    "print(correlation_with_pneumonia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the correlation data\n",
    "correlation_data = {\n",
    "    \"GLCM_Contrast\": [0.420461, -0.420461],\n",
    "    \"GLCM_Correlation\": [-0.223588, 0.223588],\n",
    "    \"NORMAL\": [1.0, -1.0],\n",
    "    \"PNEUMONIA\": [-1.0, 1.0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(correlation_data, index=[\"NORMAL\", \"PNEUMONIA\"])\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Diagnosis')\n",
    "plt.title('GLCM Feature Correlation with Diagnosis')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b889e7",
   "metadata": {},
   "source": [
    "#### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Features\n",
    "means = []\n",
    "stds = []\n",
    "skewnesses = []\n",
    "kurtoses = []\n",
    "\n",
    "for image in images:\n",
    "    \n",
    "    # Convert the image data type to uint8\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Calculate statistics of pixel intensities\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    skewness = np.mean(((image - mean) / std) ** 3)\n",
    "    kurtosis = np.mean(((image - mean) / std) ** 4)\n",
    "\n",
    "    # Append statistical features and label to respective lists\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "    skewnesses.append(skewness)\n",
    "    kurtoses.append(kurtosis)\n",
    "\n",
    "# Create a Pandas DataFrame with statistical features and labels\n",
    "import pandas as pd\n",
    "\n",
    "nb_stat = pd.DataFrame({'Mean': means, 'Std': stds, 'Skewness': skewnesses, 'Kurtosis': kurtoses})\n",
    "\n",
    "print(nb_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the results\n",
    "summary = nb_stat.describe().T\n",
    "\n",
    "# Extracting additional statistics: median, skewness and kurtosis\n",
    "summary['median'] = nb_stat.median()\n",
    "summary['skew'] = nb_stat.skew()\n",
    "summary['kurt'] = nb_stat.kurt()\n",
    "\n",
    "# Displaying the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb717f52",
   "metadata": {},
   "source": [
    "#### Concatenate Numerical Features with Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c35eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nb = pd.concat([nb_stat, nb_glcm, nb_histogram], axis=1)\n",
    "data_nb['Label'] = labels\n",
    "\n",
    "print(data_nb.columns.tolist()); print(data_nb.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade022e6",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cad30e",
   "metadata": {},
   "source": [
    "### Texture-based Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ab157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_glcm_features(image_path):\n",
    "    # Read the image\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    # Check the number of channels\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:  # RGB Image\n",
    "        gray_image = rgb2gray(image)\n",
    "    elif len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1):  # Grayscale Image\n",
    "        gray_image = image\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image format with shape {image.shape}\")\n",
    "    \n",
    "    gray_image = (gray_image * 255).astype('uint8')\n",
    "    \n",
    "    # Calculate the greycomatrix\n",
    "    glcm = graycomatrix(gray_image, [1], [0],  symmetric=True, normed=True)\n",
    "    \n",
    "    # Extract features from the GLCM\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "    return contrast, energy, homogeneity\n",
    "\n",
    "\n",
    "path = \"chest_xray\" \n",
    "folders = ['train', 'test', 'val']\n",
    "\n",
    "data = {\n",
    "    'Image': [],\n",
    "    'Class': [],\n",
    "    'Dataset': [],\n",
    "    'Contrast': [],\n",
    "    'Energy': [],\n",
    "    'Homogeneity': []\n",
    "}\n",
    "\n",
    "for dataset in tqdm(folders):\n",
    "    dataset_path = os.path.join(path, dataset)\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            contrast, energy, homogeneity = extract_glcm_features(image_path)\n",
    "            data['Image'].append(image_name)\n",
    "            data['Class'].append(class_name)\n",
    "            data['Dataset'].append(dataset)\n",
    "            data['Contrast'].append(contrast)\n",
    "            data['Energy'].append(energy)\n",
    "            data['Homogeneity'].append(homogeneity)\n",
    "\n",
    "df_glcm = pd.DataFrame(data)\n",
    "print(df_glcm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027df96d",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,          # Randomly rotate images in the range (0-40 degrees)\n",
    "    width_shift_range=0.2,      # Randomly shift images horizontally by 20% of the width\n",
    "    height_shift_range=0.2,     # Randomly shift images vertically by 20% of the height\n",
    "    shear_range=0.2,            # Randomly shear images\n",
    "    zoom_range=0.2,             # Randomly zoom inside images\n",
    "    horizontal_flip=True,       # Randomly flip images horizontally\n",
    "    fill_mode='nearest',        # Fill in newly created pixels, which can appear after a rotation or width/height shift\n",
    "    brightness_range=[0.8, 1.2] # Randomly change brightness (80-120% of original image)\n",
    ")\n",
    "\n",
    "# Assuming you have your training images in a variable called `x_train`\n",
    "# and their corresponding labels in a variable called `y_train`\n",
    "# x_train = ... (your image data here, typically shaped as [num_samples, height, width, channels])\n",
    "# y_train = ... (your labels here)\n",
    "\n",
    "# Visualize some augmented images\n",
    "# Taking one image from the dataset to demonstrate augmentation\n",
    "img = x_train[0]  # adjust this if you have a different dataset arrangement\n",
    "img = img.reshape((1,) + img.shape)\n",
    "\n",
    "# Generate batches of augmented images from the original image\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "i = 0\n",
    "for batch in datagen.flow(img, batch_size=1):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(batch[0].astype('uint8'))\n",
    "    i += 1\n",
    "    if i == 9:  # display only 9 images\n",
    "        break\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61d9ba",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289deaf",
   "metadata": {},
   "source": [
    "Among the numerical features created, Column Names '0', '1'...'255' represent the Histogram of Pixel Intensities. This causes our dataset to be extremely high in dimensionality. Before conducting analysis and modelling, we attempt to reduce dimensionality by converting histogram features to features that can justify the shape of this histogram - \"Hist_Skewness\", \"Hist_Kurtosis\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb552bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "histogram_features = range(256)\n",
    "\n",
    "# Obtain Skewness of Histogram Features\n",
    "data_nb['Hist_Skewness'] = data_nb[histogram_features].apply(lambda row: skew(row), axis=1)\n",
    "\n",
    "# Obtain Kurtosis of Histogram Features\n",
    "data_nb['Hist_Kurtosis'] = data_nb[histogram_features].apply(lambda row: kurtosis(row), axis=1)\n",
    "\n",
    "# Drop Initial Histogram Features\n",
    "data_nb = data_nb.drop(columns=histogram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode the Target \"Label\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit & Transform \"Label\"\n",
    "y_encoded = label_encoder.fit_transform(data_nb['Label'])\n",
    "\n",
    "# View mappings of encoded Labels\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Label Mapping: \")\n",
    "\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"{label}: {code}\")\n",
    "    \n",
    "# Fit Labelling into dataframe\n",
    "data_nb[\"Label\"] = y_encoded\n",
    "\n",
    "data_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdded4",
   "metadata": {},
   "source": [
    "### Implementation of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de20c4",
   "metadata": {},
   "source": [
    "To begin the Modelling, we split our data into Training, Validation and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349fd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input and Output Variables\n",
    "X = data_nb.drop(columns='Label', axis=1)\n",
    "y = data_nb['Label']\n",
    "\n",
    "# Initial Train-Test Split\n",
    "X_train, X_test2, y_train, y_test2 = train_test_split(X, y, test_size=0.30, random_state=460, shuffle =True)\n",
    "\n",
    "# Further split Test data into Validation and Test Sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test2, y_test2, test_size=0.33, random_state=460, shuffle =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9adff",
   "metadata": {},
   "source": [
    "We standardize our values to allow for easier convergence of optimization later on in the hyperparameter tuning. It also gives all the features equal importance.  \n",
    "    \n",
    "Furthermore, as Naive Bayes assumes that our features follow a normal distribution, standardization allows us to achieve Gaussian Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features as our Output has values 0 or 1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "# Initialize and train the Gaussian Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d0be9",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8d281",
   "metadata": {},
   "source": [
    "While Gaussian Naive Bayes does not have any hyperparameters compared to complex models like CNN or SVM, we can use these parameters for adjustment in our Validation Set:  \n",
    "1) Priors (class_priors) - this is prior probabilities for each class.  \n",
    "2) Var_smoothing - controls the portion of the largest variance of all features added to the variances for calculation stablility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter grid\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7],\n",
    "    'priors': [None, [0.5,0.5], [0.7,0.3], [0.9,0.1]]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with Cross-Validation (cv=5)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf04824",
   "metadata": {},
   "source": [
    "However, these best_params proved to have been overfitting the model. We can conclude that although Naive Bayes is not technically a very reliable model for this image classification problem, it could also be that insufficient numerical features were extracted to make an accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97887817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model on Combined Training & Validation Data\n",
    "X_train_val = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Create a new GNB model with the tuned hyperparameters\n",
    "model_tuned = GaussianNB(priors=None, var_smoothing = 1e-10)\n",
    "\n",
    "# Train the new GNB model on the combined training and validation data\n",
    "model_tuned.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = model_tuned.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "print(\"Accuracy on test set:\", accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae2fa3",
   "metadata": {},
   "source": [
    "#### Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de730b7",
   "metadata": {},
   "source": [
    "The evaluation will be done on a confusion matrix to visualize the Precision, Accuracy and Recall of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7627349",
   "metadata": {},
   "source": [
    "_Evaluate initial model (Pre-Hyperparameter Tuning) with better scores on test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "TP = np.sum((y_test == 1) & (y_pred == 1))\n",
    "FP = np.sum((y_test == 0) & (y_pred == 1))\n",
    "TN = np.sum((y_test == 0) & (y_pred == 0))\n",
    "FN = np.sum((y_test == 1) & (y_pred == 0))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Recall (Sensitivity)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# F1-Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (Test Data)')\n",
    "plt.show()\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad17205",
   "metadata": {},
   "source": [
    "_Evaluate tuned model (After Hyperparameter Tuning) with worse scores on test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248aa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_tuned = model_tuned.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "TP = np.sum((y_test == 1) & (y_pred_tuned == 1))\n",
    "FP = np.sum((y_test == 0) & (y_pred_tuned == 1))\n",
    "TN = np.sum((y_test == 0) & (y_pred_tuned == 0))\n",
    "FN = np.sum((y_test == 1) & (y_pred_tuned == 0))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Recall (Sensitivity)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# F1-Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (Test Data)')\n",
    "plt.show()\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361f703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
